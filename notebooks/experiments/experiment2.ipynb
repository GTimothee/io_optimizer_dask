{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "- Using hdf5 compressed file as input and splitting it into hdf5 datasets (in 1 hdf5 file but physically chunked).\n",
    "- We remarked that the reading time seemed extremely fast compared to the writing time.\n",
    "\n",
    "## Experiment 2.0.1 - Writing from hdf5 to numpy files (`stack`):\n",
    "\n",
    "Experiment setting: \n",
    "- one buffer (= 2.5GB)\n",
    "- one thread\n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "\n",
    "<img src=\"images/to_npy_stack_1thread.png\" width=\"500\">\n",
    "\n",
    "Observations:\n",
    "- reading time speed >> writing time speed\n",
    "- 2.5GB of data have been effectively loaded so the data should be uncompressed.\n",
    "- read time for 1 buffer: 1s to 2s \n",
    "- write time for 1 buffer: ~20s \n",
    "\n",
    "**Side note:** Using 1 thread here. We figured out that as the disk is sequential there is no need for us to use more threads. The memory constraint was already constraining the scheduler to run one loading task at a time but we missed the fact that the writing was still parallel, hence making seeks while writing several buffers at the same time:\n",
    "\n",
    "<img src=\"images/to_npy_stack.png\" width=\"500\">\n",
    "\n",
    "## Questions\n",
    "- Why is the reading time so fast compared to the writing time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.0.2 - What is the normal read/write speed of the computer I am working on?\n",
    "Methodology:\n",
    "**TODO**\n",
    "  \n",
    "  \n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "\n",
    "Output: \n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.1 - Does the write time contain the decompression?\n",
    "1. What is the real writing time?\n",
    "2. Does the sum read time + decompression time ~= 1s to 2s hold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1.1 - What is the real writing time?\n",
    "**Methology:**\n",
    "- creating an **uncompressed** random array of 2.5GB (2.5GB = size of 1 buffer in experiment `small`) \n",
    "- measuring the time needed to write it in a numpy file \n",
    "    - using `da.to_stack` dask function\n",
    "    - using `np.save` numpy function\n",
    "\n",
    "  \n",
    "  \n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "\n",
    "**Output of function:**\n",
    "- da.to_stack: **TODO**\n",
    "- np.save: The writing time is ~18.8s\n",
    "\n",
    "**Conclusion:**\n",
    "- The writing time in this experiment matches the writing time in the `Experiment 2.0.1` so the decompression does not seem to be included in the write time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1.2 - What is the decompression time and the time for reading the compressed file?\n",
    "**Methology:**\n",
    "- MEASURE:\n",
    "    - time to read compressed file\n",
    "    - time to uncompress file in memory\n",
    "- COMPARE:\n",
    "    - **if** time to read + time to uncompress ~= 1 to 2s (time to read in previous experiment) **then** the write time does not seem to contain decompression.\n",
    "    - to confirm this however, we need to see if the time to write uncompressed data alone is really about 18.8s (`Experiment 2.1.1` above) \n",
    "\n",
    "\n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "\n",
    "**Output of function:**\n",
    "\n",
    "**Conclusion:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step:**\n",
    "The reading time is **small** but the writing time seems normal according to previous experiments. Therefore, we want to understand what makes the reading time so small?\n",
    "\n",
    "**Question**: Why is the reading time so small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2.2 - Why is the reading time so small?\n",
    "**Hypotheses**:\n",
    "\n",
    "The reading time seems to be small because of two factors:\n",
    "1. file is compressed -> reading is fast\n",
    "2. file contains mostly zeros -> decompression is fast\n",
    "    - because file contains random data drawn from Normal(0,1) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.2.1: Reading is fast because the file is compressed. \n",
    "**Hypothesis:** Reading is fast because the file is compressed. \n",
    "    \n",
    "**Methodology:**\n",
    "0. context:\n",
    "    - create a 2.5GB compressed hdf5 filedrawn from normal(0,1) distribution\n",
    "    - create a 2.5GB uncompressed hdf5 file drawn from normal(0,1) distribution\n",
    "    \n",
    "1. measure:\n",
    "    - read time for compressed file\n",
    "    - read time for uncompressed file\n",
    "\n",
    "2. compare: \n",
    "    - **If** compressed_file_readtime << uncompressed_file_readtime **then,** hypothesis seems validated.\n",
    "    - **If** uncompressed file_readtime ~= reading time for 2.5GB of data **then** hypothesis seems validated.\n",
    "  \n",
    "  \n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "\n",
    "**Output of function:**\n",
    "\n",
    "    \n",
    "**Next step:**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.2.2: Decompression is fast because of the normal distribution.\n",
    "**Hypothesis:** Decompression is fast because it has been drawn from a normal(0,1) distribution.\n",
    "    \n",
    "**Methodology:**\n",
    "0. context:\n",
    "    - create a 2.5GB **compressed** hdf5 filedrawn from **normal**(0,1) distribution\n",
    "    - create a 2.5GB **compressed** hdf5 file drawn from **uniform** distribution\n",
    "1. measure:\n",
    "    - time to read file in main memory\n",
    "    - decompression time\n",
    "    \n",
    "    - -> see if it sums to 1~2 seconds for the normal(0,1)\n",
    "    - -> see if the decompression time is bigger for the uniform distribution\n",
    "  \n",
    "  \n",
    "- [ ] faire tourner ces tests sur bluff\n",
    "- [ ] faire plusieurs runs pour voir la variabilité\n",
    "    \n",
    "**Output of function:**\n",
    "- worst case time to load compressed file: 0.28s \n",
    "- worst case time to load data in RAM: 0.8s \n",
    "- Overall time to load data in memory from compressed file ~= 1s\n",
    "\n",
    "\n",
    "- saving time to numpy stack or npy file is almost the same\n",
    "    - 0.8s + 18.8s = 19.6s (npy file)\n",
    "    - 19.1s (npy stack)\n",
    "- gzip-based hdf5 file write time:~16s\n",
    "- uncompressed hdf5: ~82s\n",
    "    \n",
    "**Next step:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lossless compression filters\n",
    "\n",
    "For h5py `group` used in `datasets`\n",
    "\n",
    "- **GZIP filter (\"gzip\")**\n",
    "    Available with every installation of HDF5, so it’s best where portability is required. Good compression, moderate speed. compression_opts sets the compression level and may be an integer from 0 to 9, default is 4.\n",
    "- **LZF filter (\"lzf\")**\n",
    "    Available with every installation of h5py (C source code also available). Low to moderate compression, very fast. No options.\n",
    "- **SZIP filter (\"szip\")**\n",
    "    Patent-encumbered filter used in the NASA community. Not available with all installations of HDF5 due to legal reasons. Consult the HDF5 docs for filter options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the impact of hdf5 compression on clustered reads/writes\n",
    "gzip requiert de lire toutes les données pour compresser => comment fait dask alors puisquil ecrit en \"parallele\"?\n",
    "-> on ne va pas perdre en efficacité avec clustered strategy even if gzip parce que la rapidité de dask justement avec des petits buffers cest parce qu'il store en chunked donc il nous suffit de store en chunk physique aussi avec chunk size = buffer size. Peut etre meme quon peut paralléliser la compression pour que ca aille plus vite.\n",
    "\n",
    "si on veut rechunk ca ne marche pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions à traiter:\n",
    "- estce que gzip est indexé? lire la these de yongping et le papier de recherche\n",
    "- faire tests sans compression pour ne pas avoir de variabilité selon de bloc que l'on choisi "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "io_optimizer",
   "language": "python",
   "name": "io_optimizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
